{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFCuy7O6gb1K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "from google_play_scraper import Sort, reviews\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 1. SCRAPING 1000 ULASAN CHATGPT\n",
        "# ============================================\n",
        "APP_ID = 'com.openai.chatgpt'\n",
        "JUMLAH_ULASAN = 1000\n",
        "\n",
        "result, _ = reviews(\n",
        "    APP_ID,\n",
        "    lang='id',\n",
        "    country='id',\n",
        "    sort=Sort.NEWEST,\n",
        "    count=JUMLAH_ULASAN\n",
        ")\n",
        "\n",
        "df = pd.DataFrame(result)\n",
        "df = df[['content']]   # kolom ulasan saja\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 2. CLEANING TEKS\n",
        "# ============================================\n",
        "def bersihkan(teks):\n",
        "    if pd.isna(teks):\n",
        "        return \"\"\n",
        "    teks = teks.lower()\n",
        "    teks = re.sub(r'[^a-zA-Z0-9\\s]', ' ', teks)\n",
        "    teks = re.sub(r'\\s+', ' ', teks).strip()\n",
        "    return teks\n",
        "\n",
        "df['clean'] = df['content'].apply(bersihkan)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 3. LABELING SENTIMEN (Rule-based)\n",
        "# ============================================\n",
        "kata_positif = [\n",
        "    \"bagus\", \"puas\", \"cepat\", \"mantap\", \"keren\", \"baik\", \"berguna\",\n",
        "    \"akurat\", \"recommended\", \"suka\", \"lancar\", \"bantu\", \"nyaman\"\n",
        "]\n",
        "\n",
        "kata_negatif = [\n",
        "    \"buruk\", \"jelek\", \"lambat\", \"error\", \"crash\", \"mengecewakan\",\n",
        "    \"tidak bisa\", \"bug\", \"lemot\", \"parah\", \"susah\", \"tidak berfungsi\"\n",
        "]\n",
        "\n",
        "def label(teks):\n",
        "    words = teks.split()\n",
        "    pos = sum(1 for w in words if w in kata_positif)\n",
        "    neg = sum(1 for w in words if w in kata_negatif)\n",
        "\n",
        "    if pos > neg and pos > 0:\n",
        "        return \"Positif\"\n",
        "    elif neg > pos and neg > 0:\n",
        "        return \"Negatif\"\n",
        "    else:\n",
        "        return \"Netral\"\n",
        "\n",
        "df['sentimen'] = df['clean'].apply(label)\n",
        "\n",
        "TARGET_POS = 100\n",
        "TARGET_NEG = 100\n",
        "TARGET_NET = 50\n",
        "\n",
        "df_pos = df[df['sentimen'] == \"Positif\"]\n",
        "df_neg = df[df['sentimen'] == \"Negatif\"]\n",
        "df_net = df[df['sentimen'] == \"Netral\"]\n",
        "\n",
        "# fungsi sesuaikan jumlah (pakai oversampling bila kurang)\n",
        "def ambil_data(df_kat, target):\n",
        "    if len(df_kat) >= target:\n",
        "        return df_kat.sample(n=target, random_state=42)\n",
        "    else:\n",
        "        kekurangan = target - len(df_kat)\n",
        "        tambahan = df_kat.sample(n=kekurangan, replace=True, random_state=42)\n",
        "        return pd.concat([df_kat, tambahan])\n",
        "\n",
        "final_pos = ambil_data(df_pos, TARGET_POS)\n",
        "final_neg = ambil_data(df_neg, TARGET_NEG)\n",
        "final_net = ambil_data(df_net, TARGET_NET)\n",
        "\n",
        "# gabung final\n",
        "final_df = pd.concat([final_pos, final_neg, final_net]).sample(frac=1, random_state=42)\n",
        "\n",
        "print(\"\\nJumlah Akhir:\")\n",
        "print(final_df['sentimen'].value_counts())\n",
        "\n",
        "# simpan ke CSV\n",
        "final_df.to_csv(\"dataset_chatgpt_100_pos_100_neg_50_net.csv\", index=False, encoding='utf-8')\n",
        "\n",
        "print(\"\\nDataset selesai dibuat: dataset_chatgpt_100_pos_100_neg_50_net.csv\")\n",
        "print(\"Total data:\", len(final_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cax2-pqiizAK",
        "outputId": "2b680faf-fe93-411d-f944-43ef4c2dcc5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah Akhir:\n",
            "sentimen\n",
            "Negatif    100\n",
            "Positif    100\n",
            "Netral      50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset selesai dibuat: dataset_chatgpt_100_pos_100_neg_50_net.csv\n",
            "Total data: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labeling Self Training"
      ],
      "metadata": {
        "id": "xCfjd6D7jjTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Load Data Ulasan (1000 ulasan hasil scraping)\n",
        "# ----------------------------------------------------\n",
        "df_raw = df.copy()\n",
        "\n",
        "print(\"Total ulasan hasil scraping:\", len(df_raw))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Siapkan 250 Data Berlabel Manual\n",
        "#    (100 positif, 100 negatif, 50 netral)\n",
        "# ----------------------------------------------------\n",
        "df_labeled = pd.read_csv(\"dataset_chatgpt_100_pos_100_neg_50_net.csv\")\n",
        "sentiment_mapping = {\"Positif\": 2, \"Netral\": 1, \"Negatif\": 0}\n",
        "df_labeled[\"label\"] = df_labeled[\"sentimen\"].map(sentiment_mapping)\n",
        "\n",
        "print(\"Total data berlabel:\", len(df_labeled))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. Siapkan 750 Data Tidak Berlabel\n",
        "# ----------------------------------------------------\n",
        "\n",
        "df_unlabeled = df_raw.iloc[len(df_labeled):].copy()\n",
        "df_unlabeled[\"label\"] = -1  # wajib label -1 untuk self-training\n",
        "\n",
        "print(\"Total data tidak berlabel (untuk self-training):\", len(df_unlabeled))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. Gabungkan Data Berlabel + Tidak Berlabel\n",
        "# ----------------------------------------------------\n",
        "df_all = pd.concat([df_labeled[['content', 'label']], df_unlabeled[['content', 'label']]], ignore_index=True)\n",
        "\n",
        "X_text = df_all[\"content\"].astype(str)   # kolom ulasan\n",
        "y_label = df_all[\"label\"].values\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. TF-IDF Vectorization\n",
        "# ----------------------------------------------------\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_vectorized = vectorizer.fit_transform(X_text)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6. Model Dasar (Logistic Regression)\n",
        "# ----------------------------------------------------\n",
        "base_model = LogisticRegression(max_iter=1500)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 7. Self-Training Classifier\n",
        "#    threshold=0.80 → hanya mengambil prediksi yang yakin\n",
        "# ----------------------------------------------------\n",
        "self_training_model = SelfTrainingClassifier(\n",
        "    base_model,\n",
        "    threshold=0.80,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 8. Train Model Self-Training\n",
        "# ----------------------------------------------------\n",
        "print(\"\\n⚡ Training Self-Training Classifier...\")\n",
        "self_training_model.fit(X_vectorized.toarray(), y_label)\n",
        "\n",
        "print(\"\\nSelf-training selesai!\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 9. Hasil: Ambil Label Baru dari 750 Ulasan\n",
        "# ----------------------------------------------------\n",
        "df_all[\"predicted_label\"] = self_training_model.predict(X_vectorized.toarray())\n",
        "\n",
        "# Hanya ambil data yang awalnya tidak berlabel (-1)\n",
        "df_new_labels = df_all[df_all[\"label\"] == -1][[\"content\", \"predicted_label\"]]\n",
        "\n",
        "# Convert numerical predicted_label back to sentiment strings for output if desired\n",
        "reverse_sentiment_mapping = {2: \"Positif\", 1: \"Netral\", 0: \"Negatif\"}\n",
        "df_new_labels[\"predicted_sentimen\"] = df_new_labels[\"predicted_label\"].map(reverse_sentiment_mapping)\n",
        "\n",
        "print(\"\\nJumlah ulasan yang berhasil dilabel otomatis:\", len(df_new_labels))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 10. Simpan hasil Self-Training\n",
        "# ----------------------------------------------------\n",
        "df_new_labels.to_csv(\"hasil_label_self_training_750.csv\", index=False, encoding=\"utf-8\")\n",
        "df_all.to_csv(\"gabungan_1000_dengan_label.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\n✔ File berhasil disimpan:\")\n",
        "print(\" - hasil_label_self_training_750.csv\")\n",
        "print(\" - gabungan_1000_dengan_label.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXVccooXjLFZ",
        "outputId": "d5dca835-b53e-47b6-d5b1-bfc8ecd4253d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total ulasan hasil scraping: 1000\n",
            "Total data berlabel: 250\n",
            "Total data tidak berlabel (untuk self-training): 750\n",
            "\n",
            "⚡ Training Self-Training Classifier...\n",
            "End of iteration 1, added 127 new labels.\n",
            "End of iteration 2, added 30 new labels.\n",
            "End of iteration 3, added 9 new labels.\n",
            "End of iteration 4, added 4 new labels.\n",
            "\n",
            "Self-training selesai!\n",
            "\n",
            "Jumlah ulasan yang berhasil dilabel otomatis: 750\n",
            "\n",
            "✔ File berhasil disimpan:\n",
            " - hasil_label_self_training_750.csv\n",
            " - gabungan_1000_dengan_label.csv\n"
          ]
        }
      ]
    }
  ]
}